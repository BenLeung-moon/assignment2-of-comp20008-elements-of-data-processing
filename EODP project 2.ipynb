{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datasets and accompanying files\n",
    "DATA_PATH = \"Users\\梁浩弘\\OneDrive\\学习\\UniMelb\\Year2\\Semester2\\Eodp\\Ass2\\a2\"\n",
    "\n",
    "for root, dirs, files in os.walk(DATA_PATH):\n",
    "        level = root.replace(DATA_PATH, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the dataset of each region and convert into dataframe \n",
    "f = open(DATA_PATH+'/games/EUmatch.csv', \"r\")\n",
    "EU = pd.read_csv(f)\n",
    "EU = pd.DataFrame(EU)\n",
    "\n",
    "f = open(DATA_PATH+'/games/KRmatch.csv', \"r\")\n",
    "KR = pd.read_csv(f)\n",
    "KR = pd.DataFrame(KR)\n",
    "\n",
    "f = open(DATA_PATH+'/games/NAmatch.csv', \"r\")\n",
    "NA = pd.read_csv(f)\n",
    "NA = pd.DataFrame(NA)\n",
    "\n",
    "# step 1： concat the 3 dataset into 1 dataframe\n",
    "info = pd.concat([EU, KR, NA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: drop the nah value in dataframe\n",
    "info = info.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: using convert type function to change string into unique int \n",
    "info=info.copy()\n",
    "# change the category of the data into digital\n",
    "def convert_type(data,col_list):\n",
    "    columns = col_list\n",
    "    data[columns] = data[columns].astype(\"category\")\n",
    "    for col in columns:\n",
    "        data[col]=data[col].cat.codes\n",
    "    return data\n",
    "\n",
    "info = convert_type(info,[\"champion\",\"side\",\"role\",\"minions_killed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: remove the outlier by using the quantile \n",
    "\n",
    "from matplotlib.lines import lineStyles\n",
    "\n",
    "\n",
    "info = info.sort_values(\"kda\")\n",
    "# draw boxplot of kda\n",
    "info[\"kda\"].plot.box(title=\"Boxplot of KDA Before Removing Outliers\")\n",
    "plt.show()\n",
    "# calculate quartiles\n",
    "kdaAve = info['kda'].mean()\n",
    "kdaQ1 = info['kda'].quantile(q=0.25)\n",
    "kdaQ3 = info['kda'].quantile(q=0.75)\n",
    "kdaIQR = kdaQ3 - kdaQ1\n",
    "\n",
    "# show upper outliers and lower outliers\n",
    "print(\"upper outlier is %f\", kdaQ3 + 3 * kdaIQR)\n",
    "print(\"lower outlier is %f\", kdaQ1 - 3 * kdaIQR)\n",
    "\n",
    "# drop outliers\n",
    "info_remove_out = info.drop(info[(info['kda'] < (kdaQ1 - 3 * kdaIQR)) | (info['kda'] > (kdaQ3 + 3 * kdaIQR))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "level = info_remove_out['level']\n",
    "plt.hist(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "info_remove_out.drop(info_remove_out [info_remove_out['level'] < 10].index, inplace = True)\n",
    "\n",
    "print(info_remove_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary Analysis (here contain the feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap before remove outlier \n",
    "plt.figure(figsize = (16,9))\n",
    "corr = info.corr()\n",
    "sns.heatmap(corr, cmap= 'coolwarm', annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap after remove outlier \n",
    "plt.figure(figsize = (16,9))\n",
    "corr = info_remove_out.corr()\n",
    "sns.heatmap(corr, cmap= 'coolwarm', annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the dataset using background information \n",
    "# (like we remove kill, death, assist as they are directly depended on kda)\n",
    "df = info_remove_out\n",
    "df = df.drop([\"kills\", \"deaths\", \"assists\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the corraltion which is lower than 0.05\n",
    "corrlation = df.corr(method='pearson')['kda'].sort_values()\n",
    "abs_corr = abs(corrlation).sort_values()\n",
    "print(abs_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the less correlated features \n",
    "df = df.drop([ \"role\", \"d_spell\", \"f_spell\", \"side\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spilt into training set and test set\n",
    "X=df.drop(columns='kda')\n",
    "Y=df['kda']\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y,test_size=0.2, random_state = 0)\n",
    "\n",
    "plt.hist(Y,bins=100)\n",
    "plt.xlabel(\"KDA score\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Elimination (for feature selection)\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(Y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "selected_features_BE = cols\n",
    "print(selected_features_BE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "# draw scatter plot \n",
    "\n",
    "ax.scatter(X_train['damage_objectives'], Y_train, color='Purple')\n",
    "ax.set_xlabel('damage_objective')\n",
    "ax.set_ylabel('TOTALDEMAND')\n",
    "\n",
    "z = np.polyfit(X_train['damage_objectives'], Y_train, 1)\n",
    "p = np.poly1d(z)\n",
    "#add trendline to plot\n",
    "plt.plot(X_train['damage_objectives'], p(X_train['damage_objectives']), color = 'red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model\n",
    "reg = LinearRegression().fit(X_train,Y_train)\n",
    "cv = RepeatedKFold(n_splits = 10,n_repeats=3, random_state=0)\n",
    "r_sq_1 = reg.score(X_train,Y_train)\n",
    "r_sq_2 = reg.score(X_test,Y_test)\n",
    "\n",
    "print('train', r_sq_1)\n",
    "print('test', r_sq_2)\n",
    "#print(cross_val_score(reg, X_train, Y_train, cv=5).mean())\n",
    "\n",
    "# linear regression model MSE\n",
    "Y_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(Y_test,Y_pred)\n",
    "rmse = mse**.5\n",
    "print('mse', mse)\n",
    "print('rmse', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree regression\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt \n",
    "treebase = DecisionTreeRegressor(random_state=0)\n",
    "treebase.fit(X_train, Y_train)\n",
    "print(\"train \", treebase.score(X_train, Y_train))\n",
    "print(\"test\", treebase.score(X_test, Y_test))\n",
    "print(cross_val_score(treebase, X_train, Y_train, cv=5).mean())\n",
    "\n",
    "\n",
    "# regression tree\n",
    "clf = tree.DecisionTreeRegressor(max_depth=3)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"test\",clf.score(X_test, Y_test))\n",
    "plt.figure(figsize=(10,10))\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest regression\n",
    "RF = RandomForestRegressor(n_estimators=100)\n",
    "RF.fit(X_train, Y_train)\n",
    "print(\"train \", RF.score(X_train, Y_train))\n",
    "print(\"test\",RF.score(X_test, Y_test))\n",
    "print(cross_val_score(RF, X_train, Y_train, cv=5).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance \n",
    "feature_names = [f\"{name}\" for name in X_train.head()]\n",
    "importances = RF.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in RF.estimators_], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importance graph and values\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "print(forest_importances)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83e1d904f14d6b28489adf2eb47c01be3df943fe4d14b3ab1b82c1a07d1f7302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
